{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "LEARNING_RATE = 1e-5\n",
    "INPUT_NODE = 24\n",
    "OUTPUT_NODE = 10  # 这里注意修改=============================\n",
    "\n",
    "LAYER_NODE = [200, 100]\n",
    "BATCH_SIZE = 50\n",
    "MAX_EPOCH = 1000000\n",
    "MODE_NUM = MAX_EPOCH / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define converts(字典)  \n",
    "def Name_label(s):\n",
    "#     it = {b'cjn':0, b'lmx':1, b'yxd':2, b'zzy':3 }\n",
    "#     return it[s]\n",
    "    return int(s) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '../gen_data/dynamic_features.csv'\n",
    "column_name = ['A_pos_peak_time', 'MAX_pos_peak_time', 'MIN_pos_peak_time', 'A_neg_peak_time', 'MAX_neg_peak_time', 'MIN_neg_peak_time',\n",
    " 'A_close time', 'MAX_close time', 'MIN_close time', 'A_open time', 'MAX_open time', 'MIN_open time', 'A_blink time', \n",
    " 'MAX_blink time', 'MIN_blink time', 'A_peak ratio', 'MAX_peak ratio', 'MIN_peak ratio', 'A_peak dist ratio', 'MAX_peak dist ratio', \n",
    " 'MIN_peak dist ratio', 'A_area ratio', 'MAX_area ratio', 'MIN_area ratio', 'Label']\n",
    "data = pd.read_csv(csv_path)\n",
    "data.head()\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=25)\n",
    "for train_index, test_index in split.split(data, data[\"Label\"]): \n",
    "    strat_train_set = data.loc[train_index] \n",
    "    strat_test_set = data.loc[test_index]\n",
    "strat_train_set = np.array(strat_train_set)\n",
    "strat_test_set = np.array(strat_test_set)\n",
    "train_data,train_label = np.split(strat_train_set,indices_or_sections=(24,),axis=1) #x为数据，y为标签  前24个为特征，最后一个为标签\n",
    "test_data, test_label = np.split(strat_test_set,indices_or_sections=(24,),axis=1) #x为数据，y为标签  前24个为特征，最后一个为标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one-hot covert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tfp36\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "d:\\tfp36\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder()\n",
    "# print(train_label.reshape(-1,))\n",
    "train_label_ = encoder.fit_transform(train_label.reshape(-1,1))\n",
    "train_label = np.array(train_label_.toarray())\n",
    "\n",
    "test_label_ = encoder.fit_transform(test_label.reshape(-1,1))\n",
    "test_label = np.array(test_label_.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(a))\n",
    "# print(np.shape(b))\n",
    "# print(np.shape(train_label_2.toarray()))\n",
    "# print(np.shape(test_label_2.toarray()))\n",
    "# train_label_2.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def createNetwork():\n",
    "#     定义神经网络的参数和偏置\n",
    "    w_fc1 = weight_variable([INPUT_NODE, LAYER_NODE[0]])\n",
    "    b_fc1 = bias_variable([LAYER_NODE[0]])\n",
    "    \n",
    "    w_fc2 = weight_variable([LAYER_NODE[0], LAYER_NODE[1]])\n",
    "    b_fc2 = bias_variable([LAYER_NODE[1]])\n",
    "    \n",
    "#     w_fc3 = weight_variable([LAYER_NODE[1], LAYER_NODE[2]])\n",
    "#     b_fc3 = bias_variable([LAYER_NODE[2]])\n",
    "    \n",
    "    w_fc3 = weight_variable([LAYER_NODE[1], OUTPUT_NODE])\n",
    "    b_fc3 = bias_variable([OUTPUT_NODE])\n",
    "    \n",
    "#     输入层\n",
    "    s = tf.placeholder(\"float\", [None, INPUT_NODE])\n",
    "    \n",
    "#     隐藏层\n",
    "    fc1 = tf.nn.relu(tf.matmul(s, w_fc1) + b_fc1)\n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, w_fc2) + b_fc2)\n",
    "#     fc3 = tf.nn.relu(tf.matmul(fc2, w_fc3) + b_fc3)\n",
    "    readout = tf.nn.softmax(tf.matmul(fc2, w_fc3) + b_fc3)\n",
    "    \n",
    "    tf.add_to_collection(tf.GraphKeys.WEIGHTS, w_fc1)\n",
    "    tf.add_to_collection(tf.GraphKeys.WEIGHTS, w_fc2)\n",
    "    tf.add_to_collection(tf.GraphKeys.WEIGHTS, w_fc3)\n",
    "\n",
    "    \n",
    "    print(\"end of create network\")\n",
    "    return s, readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNetwork(s, readout, sess, max_epoch=MAX_EPOCH, lr=LEARNING_RATE):\n",
    "    y_ = tf.placeholder(\"float\", [None, OUTPUT_NODE])\n",
    "    x = s\n",
    "    y = readout\n",
    "    \n",
    "    train_data_length = len(train_data)\n",
    "    print(\"train_data_length: \", train_data_length)\n",
    "    \n",
    "#     cross_entropy = -tf.reduce_mean(y_*tf.log(y))\n",
    "#     避免因为出现 0*log(0) 导致的Nan\n",
    "    cross_entropy = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y,1e-10,1.0)))\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=5.0 / train_data_length)\n",
    "    reg_term = tf.contrib.layers.apply_regularization(regularizer)\n",
    "    cross_entropy_ = cross_entropy + reg_term\n",
    "    \n",
    "    \n",
    "    y_argmax = tf.argmax(input=y,axis=1)\n",
    "    y_argmax_ = tf.argmax(input=y_,axis=1)\n",
    "    \n",
    "    \n",
    "    eq_ = tf.equal(y_argmax, y_argmax_)\n",
    "    eq_ = tf.cast(eq_, dtype=tf.float32) # 这里不能为int\n",
    "    acc = tf.reduce_mean(eq_)\n",
    "    \n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy_)\n",
    "    \n",
    "    # 用于加载或保存网络参数\n",
    "    saver = tf.train.Saver(max_to_keep=3)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Could not find old network weights\")\n",
    "        \n",
    "    \n",
    "#     data_path = '../gen_data/dynamic_feature-2-26.data'\n",
    "#     data = np.loadtxt(data_path, dtype=float, delimiter=',', converters={24:Name_label})\n",
    "#     # 划分数据与标签\n",
    "#     total_data, total_label = np.split(data,indices_or_sections=(24,),axis=1) # 前24个为特征，最后一个为标签\n",
    "\n",
    "    # 将标签转化成 one-hot形式\n",
    "    # 行向量转列向量\n",
    "#     print(total_label)\n",
    "#     total_label = np.array(total_label).reshape(len(total_label), -1)\n",
    "#     enc = OneHotEncoder()\n",
    "#     enc.fit(total_label)\n",
    "#     total_label = enc.transform(total_label).toarray()\n",
    "# #     print(\"============\")\n",
    "# #     print(total_label)\n",
    "#     print(\"one-hot shape:\", np.shape(total_label))\n",
    "    \n",
    "    \n",
    "#     # 指定随机数种子，确保每次划分结果一致\n",
    "#     train_data,test_data,train_label,test_label = train_test_split(total_data, total_label, random_state=42, \n",
    "#                                                                    train_size=0.8, test_size=0.2) #sklearn.model_selection.\n",
    "    \n",
    "    training_epoch = 0\n",
    "\n",
    "    \n",
    "    acc_list =[]\n",
    "    val_list =[]\n",
    "    while training_epoch <= max_epoch:\n",
    "        # 数据的选取\n",
    "        valid_indices = np.random.choice(train_data_length, [BATCH_SIZE], replace=False)\n",
    "        batch_train_data = train_data[valid_indices]\n",
    "        batch_train_label = train_label[valid_indices]\n",
    "        \n",
    "#         debug ==========\n",
    "#         print(\"batch_train_data\", np.shape(batch_train_data))\n",
    "#         print(\"batch_train_label\", np.shape(batch_train_label))\n",
    "        \n",
    "        loss, _ = sess.run([cross_entropy_, train_step], feed_dict={s: batch_train_data, y_: batch_train_label})\n",
    "        \n",
    "        if training_epoch % MODE_NUM == 0:\n",
    "            # validation set\n",
    "#             valid_set_indices = np.random.choice(train_data_length, [BATCH_SIZE], replace=False)\n",
    "#             valid_set_data = train_data[valid_set_indices]\n",
    "#             valid_set_label = train_label[valid_set_indices]\n",
    "            \n",
    "            acc_ = sess.run([acc,], feed_dict={s: test_data, y_: test_label})\n",
    "            acc_ = acc_[0]\n",
    "            \n",
    "            val_ = sess.run([acc,], feed_dict={s: train_data, y_: train_label})\n",
    "            val_ = val_[0]\n",
    "            \n",
    "            print(\"epoch=%d, loss=%.5f, valid_acc=%.5f, test_acc=%.5f\" % (training_epoch, loss, val_, acc_))\n",
    "            acc_list.append(acc_)\n",
    "            val_list.append(val_)\n",
    "        \n",
    "#         每进行500次迭代，保留一下网络参数\n",
    "        if (training_epoch + 1) % 100000 == 0:\n",
    "            saver.save(sess, 'saved_networks/od', global_step=training_epoch)\n",
    "        \n",
    "        training_epoch += 1\n",
    "    \n",
    "    print(\"This time training has finished! Total training epoch are %d\" % (training_epoch - 1, ))\n",
    "    \n",
    "    return acc_list, val_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNetwork():\n",
    "    a = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyst_plot(lst, delta=101):\n",
    "#     训练过程中accuracy的变化过程\n",
    "    epoch = np.linspace(0, MAX_EPOCH, delta)\n",
    "    acc, val = np.array(lst)\n",
    "    \n",
    "    plt.title(\"Learning Curve\")\n",
    "    # plt.legend(loc=\"upper right\", fontsize=14)  # not shown in the book\n",
    "    plt.xlabel(\"Training set size\", fontsize=14)  # not shown\n",
    "    plt.ylabel(\"accuracy\", fontsize=14)  # not shown\n",
    "\n",
    "    plt.plot(epoch, acc, color='red', label='train')\n",
    "    plt.plot(epoch, val, color='blue', label='val')\n",
    "    plt.legend(loc=\"best\", fontsize=14)   # not shown in the book\n",
    "\n",
    "#     plt.show()\n",
    "    plt.savefig(\"../myplot/mlp_learning_curve.png\")  # 图像保存\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end of create network\n",
      "train_data_length:  210\n",
      "WARNING:tensorflow:From d:\\tfp36\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Could not find old network weights\n",
      "epoch=0, loss=0.25293, valid_acc=0.09524, test_acc=0.07547\n",
      "epoch=10000, loss=0.17945, valid_acc=0.49048, test_acc=0.52830\n",
      "epoch=20000, loss=0.18255, valid_acc=0.53810, test_acc=0.60377\n",
      "epoch=30000, loss=0.16043, valid_acc=0.54286, test_acc=0.62264\n",
      "epoch=40000, loss=0.17296, valid_acc=0.58095, test_acc=0.62264\n",
      "epoch=50000, loss=0.15521, valid_acc=0.57619, test_acc=0.62264\n",
      "epoch=60000, loss=0.16042, valid_acc=0.59048, test_acc=0.62264\n",
      "epoch=70000, loss=0.16292, valid_acc=0.59524, test_acc=0.62264\n",
      "epoch=80000, loss=0.14873, valid_acc=0.61429, test_acc=0.64151\n",
      "epoch=90000, loss=0.16296, valid_acc=0.62381, test_acc=0.64151\n",
      "epoch=100000, loss=0.16226, valid_acc=0.62381, test_acc=0.66038\n",
      "epoch=110000, loss=0.15071, valid_acc=0.62857, test_acc=0.66038\n",
      "epoch=120000, loss=0.16219, valid_acc=0.63333, test_acc=0.64151\n",
      "epoch=130000, loss=0.15564, valid_acc=0.65714, test_acc=0.67925\n",
      "epoch=140000, loss=0.13974, valid_acc=0.66190, test_acc=0.69811\n",
      "epoch=150000, loss=0.15467, valid_acc=0.69524, test_acc=0.69811\n",
      "epoch=160000, loss=0.12648, valid_acc=0.71905, test_acc=0.67925\n",
      "epoch=170000, loss=0.14329, valid_acc=0.73810, test_acc=0.69811\n",
      "epoch=180000, loss=0.13644, valid_acc=0.72857, test_acc=0.67925\n",
      "epoch=190000, loss=0.15531, valid_acc=0.74286, test_acc=0.69811\n",
      "epoch=200000, loss=0.13869, valid_acc=0.75238, test_acc=0.71698\n",
      "epoch=210000, loss=0.14565, valid_acc=0.74286, test_acc=0.67925\n",
      "epoch=220000, loss=0.14649, valid_acc=0.75238, test_acc=0.69811\n",
      "epoch=230000, loss=0.13828, valid_acc=0.76667, test_acc=0.67925\n",
      "epoch=240000, loss=0.14484, valid_acc=0.76667, test_acc=0.67925\n",
      "epoch=250000, loss=0.15350, valid_acc=0.76190, test_acc=0.69811\n",
      "epoch=260000, loss=0.14708, valid_acc=0.76667, test_acc=0.67925\n",
      "epoch=270000, loss=0.14756, valid_acc=0.78095, test_acc=0.69811\n",
      "epoch=280000, loss=0.14097, valid_acc=0.78095, test_acc=0.66038\n",
      "epoch=290000, loss=0.13370, valid_acc=0.77143, test_acc=0.69811\n",
      "epoch=300000, loss=0.14243, valid_acc=0.77143, test_acc=0.71698\n",
      "epoch=310000, loss=0.13637, valid_acc=0.77619, test_acc=0.71698\n",
      "epoch=320000, loss=0.14504, valid_acc=0.77619, test_acc=0.69811\n",
      "epoch=330000, loss=0.15059, valid_acc=0.77619, test_acc=0.71698\n",
      "epoch=340000, loss=0.13589, valid_acc=0.78095, test_acc=0.71698\n",
      "epoch=350000, loss=0.13739, valid_acc=0.78571, test_acc=0.69811\n",
      "epoch=360000, loss=0.13837, valid_acc=0.78571, test_acc=0.71698\n",
      "epoch=370000, loss=0.13205, valid_acc=0.78571, test_acc=0.71698\n",
      "epoch=380000, loss=0.12694, valid_acc=0.79524, test_acc=0.71698\n",
      "epoch=390000, loss=0.13052, valid_acc=0.80000, test_acc=0.71698\n",
      "epoch=400000, loss=0.15007, valid_acc=0.79048, test_acc=0.71698\n",
      "epoch=410000, loss=0.14240, valid_acc=0.79524, test_acc=0.71698\n",
      "epoch=420000, loss=0.13000, valid_acc=0.80000, test_acc=0.71698\n",
      "epoch=430000, loss=0.12400, valid_acc=0.80952, test_acc=0.69811\n",
      "epoch=440000, loss=0.11805, valid_acc=0.79524, test_acc=0.69811\n",
      "epoch=450000, loss=0.14997, valid_acc=0.80000, test_acc=0.71698\n",
      "epoch=460000, loss=0.14390, valid_acc=0.80476, test_acc=0.71698\n",
      "epoch=470000, loss=0.13731, valid_acc=0.79524, test_acc=0.71698\n",
      "epoch=480000, loss=0.14925, valid_acc=0.80000, test_acc=0.71698\n",
      "epoch=490000, loss=0.13711, valid_acc=0.80476, test_acc=0.71698\n",
      "epoch=500000, loss=0.13901, valid_acc=0.82381, test_acc=0.67925\n",
      "epoch=510000, loss=0.13030, valid_acc=0.79524, test_acc=0.69811\n",
      "epoch=520000, loss=0.12757, valid_acc=0.80952, test_acc=0.73585\n",
      "epoch=530000, loss=0.12416, valid_acc=0.80952, test_acc=0.69811\n",
      "epoch=540000, loss=0.14420, valid_acc=0.80952, test_acc=0.71698\n",
      "epoch=550000, loss=0.13330, valid_acc=0.80476, test_acc=0.69811\n",
      "epoch=560000, loss=0.13507, valid_acc=0.81905, test_acc=0.73585\n",
      "epoch=570000, loss=0.13161, valid_acc=0.81905, test_acc=0.71698\n",
      "epoch=580000, loss=0.14558, valid_acc=0.82381, test_acc=0.69811\n",
      "epoch=590000, loss=0.13943, valid_acc=0.82857, test_acc=0.71698\n",
      "epoch=600000, loss=0.13371, valid_acc=0.82381, test_acc=0.73585\n",
      "epoch=610000, loss=0.12779, valid_acc=0.81905, test_acc=0.67925\n",
      "epoch=620000, loss=0.12712, valid_acc=0.81429, test_acc=0.69811\n",
      "epoch=630000, loss=0.12727, valid_acc=0.82857, test_acc=0.71698\n",
      "epoch=640000, loss=0.13037, valid_acc=0.81905, test_acc=0.71698\n",
      "epoch=650000, loss=0.13173, valid_acc=0.82381, test_acc=0.71698\n",
      "epoch=660000, loss=0.12858, valid_acc=0.82857, test_acc=0.73585\n",
      "epoch=670000, loss=0.13525, valid_acc=0.82857, test_acc=0.71698\n",
      "epoch=680000, loss=0.12900, valid_acc=0.82857, test_acc=0.73585\n",
      "epoch=690000, loss=0.13519, valid_acc=0.82381, test_acc=0.69811\n",
      "epoch=700000, loss=0.12964, valid_acc=0.82857, test_acc=0.71698\n",
      "epoch=710000, loss=0.13012, valid_acc=0.82381, test_acc=0.69811\n",
      "epoch=720000, loss=0.12518, valid_acc=0.83810, test_acc=0.69811\n",
      "epoch=730000, loss=0.13286, valid_acc=0.84286, test_acc=0.69811\n",
      "epoch=740000, loss=0.13368, valid_acc=0.82857, test_acc=0.71698\n",
      "epoch=750000, loss=0.14415, valid_acc=0.83333, test_acc=0.71698\n",
      "epoch=760000, loss=0.12135, valid_acc=0.83810, test_acc=0.73585\n",
      "epoch=770000, loss=0.12712, valid_acc=0.84286, test_acc=0.71698\n",
      "epoch=780000, loss=0.13247, valid_acc=0.83810, test_acc=0.71698\n",
      "epoch=790000, loss=0.13244, valid_acc=0.83333, test_acc=0.69811\n",
      "epoch=800000, loss=0.11584, valid_acc=0.84762, test_acc=0.73585\n",
      "epoch=810000, loss=0.13311, valid_acc=0.83810, test_acc=0.73585\n",
      "epoch=820000, loss=0.14083, valid_acc=0.82857, test_acc=0.71698\n",
      "epoch=830000, loss=0.12520, valid_acc=0.84286, test_acc=0.71698\n",
      "epoch=840000, loss=0.11774, valid_acc=0.83810, test_acc=0.75472\n",
      "epoch=850000, loss=0.13341, valid_acc=0.83810, test_acc=0.73585\n",
      "epoch=860000, loss=0.13907, valid_acc=0.84286, test_acc=0.71698\n",
      "epoch=870000, loss=0.14153, valid_acc=0.84762, test_acc=0.73585\n",
      "epoch=880000, loss=0.13163, valid_acc=0.83333, test_acc=0.71698\n",
      "epoch=890000, loss=0.15200, valid_acc=0.83810, test_acc=0.73585\n",
      "epoch=900000, loss=0.13255, valid_acc=0.83333, test_acc=0.69811\n",
      "epoch=910000, loss=0.13591, valid_acc=0.82381, test_acc=0.71698\n",
      "epoch=920000, loss=0.13750, valid_acc=0.84286, test_acc=0.71698\n",
      "epoch=930000, loss=0.12869, valid_acc=0.84762, test_acc=0.71698\n",
      "epoch=940000, loss=0.12579, valid_acc=0.82381, test_acc=0.73585\n",
      "epoch=950000, loss=0.12971, valid_acc=0.83333, test_acc=0.71698\n",
      "epoch=960000, loss=0.13733, valid_acc=0.84762, test_acc=0.75472\n",
      "epoch=970000, loss=0.13473, valid_acc=0.83333, test_acc=0.73585\n",
      "epoch=980000, loss=0.13319, valid_acc=0.82857, test_acc=0.73585\n",
      "epoch=990000, loss=0.13498, valid_acc=0.83810, test_acc=0.71698\n",
      "epoch=1000000, loss=0.13997, valid_acc=0.82381, test_acc=0.73585\n",
      "This time training has finished! Total training epoch are 1000000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    s, readout = createNetwork()\n",
    "    sess = tf.InteractiveSession()\n",
    "    mlst = trainNetwork(s, readout, sess)\n",
    "    analyst_plot(mlst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
